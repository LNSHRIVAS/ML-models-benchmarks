{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "967077b3-3838-4fdf-849c-d8a73334e669",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION ON A LARGE DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d346ca0-13f6-47da-9e8a-3ffba2e4b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on /CPU:0...\n",
      "  Accuracy       : 0.8428\n",
      "  Training Time  : 76.1190 s\n",
      "  Inference Time : 3.2941 s\n",
      "\n",
      "Running on /GPU:0...\n",
      "  Accuracy       : 0.8423\n",
      "  Training Time  : 109.4842 s\n",
      "  Inference Time : 4.6298 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Generate synthetic large dataset\n",
    "N_SAMPLES = 500000     # 5 million rows\n",
    "N_FEATURES = 200          # 200 features\n",
    "N_CLASSES = 2             # Binary classification\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=N_SAMPLES,\n",
    "                           n_features=N_FEATURES,\n",
    "                           n_informative=150,\n",
    "                           n_redundant=25,\n",
    "                           n_classes=N_CLASSES,\n",
    "                           random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Convert to float32 for GPU efficiency\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Define function to train and evaluate model\n",
    "def run_logistic_regression(device_name):\n",
    "    with tf.device(device_name):\n",
    "        # Define logistic regression model\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        print(f\"Running on {device_name}...\")\n",
    "\n",
    "        # Start training timer\n",
    "        start_train = time.time()\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0)\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        # Start inference timer\n",
    "        start_test = time.time()\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_time = time.time() - start_test\n",
    "\n",
    "        # Print results\n",
    "        print(f\"  Accuracy       : {acc:.4f}\")\n",
    "        print(f\"  Training Time  : {train_time:.4f} s\")\n",
    "        print(f\"  Inference Time : {test_time:.4f} s\\n\")\n",
    "\n",
    "# 3. Run on CPU\n",
    "run_logistic_regression(\"/CPU:0\")\n",
    "\n",
    "# 4. Run on GPU (if available)\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    run_logistic_regression(\"/GPU:0\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72164160-d63a-4c16-bff1-7da68c5897bb",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORK USING GPU AND CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48358f10-c984-4027-a66c-46d16eea850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 2s 0us/step\n",
      "\n",
      "Running on /CPU:0...\n",
      "  Accuracy       : 0.7168\n",
      "  Training Time  : 157.57 s\n",
      "  Inference Time : 2.14 s\n",
      "\n",
      "Running on /GPU:0...\n",
      "  Accuracy       : 0.7119\n",
      "  Training Time  : 45.62 s\n",
      "  Inference Time : 2.50 s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Load and preprocess CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "# 2. Define CNN model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_cifar(device_name):\n",
    "    with tf.device(device_name):\n",
    "        model = create_model()\n",
    "        print(f\"\\nRunning on {device_name}...\")\n",
    "\n",
    "        start_train = time.time()\n",
    "        model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        start_test = time.time()\n",
    "        loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "        test_time = time.time() - start_test\n",
    "\n",
    "        print(f\"  Accuracy       : {acc:.4f}\")\n",
    "        print(f\"  Training Time  : {train_time:.2f} s\")\n",
    "        print(f\"  Inference Time : {test_time:.2f} s\")\n",
    "\n",
    "\n",
    "run_cifar(\"/CPU:0\")\n",
    "\n",
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    run_cifar(\"/GPU:0\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4052ff-ecbe-4da3-826d-6adbcdfe6ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
